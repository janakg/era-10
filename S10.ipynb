{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/janakg/era-s5/blob/main/S5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PlbomWY3RSq"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the repository and the target directory\n",
    "# repo_url = 'https://github.com/janakg/era-s9.git'\n",
    "# target_dir = '/content/era-s9'\n",
    "\n",
    "# # Check if the directory already exists\n",
    "# if not os.path.exists(target_dir):\n",
    "#     # If it doesn't exist, clone the repo\n",
    "#     !git clone {repo_url}\n",
    "# else:\n",
    "#     # If it exists, 'cd' into the directory and pull the latest changes\n",
    "#     %cd {target_dir}\n",
    "#     !git pull\n",
    "\n",
    "# # Add the repository's directory to the system path\n",
    "# import sys\n",
    "# sys.path.append(target_dir)\n",
    "\n",
    "# Import all utils functions\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94BxVVBP3WwS",
    "outputId": "46605080-bcaa-4042-8b60-2b0aec71a6b4"
   },
   "outputs": [],
   "source": [
    "# CUDA?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA Available?\", use_cuda)\n",
    "\n",
    "\n",
    "# For reproducibility. SEED Random functions\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.247, 0.243, 0.261]\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Normalize(mean, std),\n",
    "    A.RandomCrop(32, 32, p=4.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.CoarseDropout(max_holes=1, max_height=8, max_width=8, min_holes=1, min_height=8, min_width=8, fill_value=mean, mask_fill_value=None),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class AlbumentationsCIFAR10Wrapper(Dataset):\n",
    "    def __init__(self, root='./data', train=True, download=True, transform=None):\n",
    "        self.data = datasets.CIFAR10(root=root, train=train, download=download)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        img = np.array(img)  # PIL Image to numpy array\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpshQ2Ug38m2"
   },
   "outputs": [],
   "source": [
    "# Train Phase transformations\n",
    "# train_transforms = transforms.Compose([\n",
    "#                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "#                                        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "#                                        transforms.RandomHorizontalFlip(),\n",
    "#                                        transforms.RandomRotation(15),\n",
    "#                                        transforms.ToTensor(),\n",
    "#                                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "#                                        ])\n",
    "\n",
    "# Test Phase transformations\n",
    "test_transforms = transforms.Compose([\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB79ZYW13-AO",
    "outputId": "f375e7e7-b967-4ca5-cfad-a816b8f58a8a"
   },
   "outputs": [],
   "source": [
    "train_data = AlbumentationsCIFAR10Wrapper(root='./data', train=True, \n",
    "                                          download=True, transform=train_transforms)\n",
    "\n",
    "test_data = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avCKK1uL4A68"
   },
   "outputs": [],
   "source": [
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "batch_size=512\n",
    "dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True) if use_cuda else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test_data, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "Hx7QkLcw4Epc",
    "outputId": "f300f2b7-1a0a-4a67-d541-fe8b9c525b86"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "import torchvision\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "# # Call the util function to show a batch of images\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure()\n",
    "# show_batch_images(plt, train_loader, 12, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHBolvMH4F8y"
   },
   "outputs": [],
   "source": [
    "# model imported from a module\n",
    "from model import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7du4zM474LvT"
   },
   "outputs": [],
   "source": [
    "# Data to plot accuracy and loss graphs\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_lr_finder\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Owqiet9M4TV7",
    "outputId": "f5c847c7-642b-4773-becf-70168897809e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.0686 Batch_id=97 Accuracy=37.46: 100%|██████████| 98/98 [00:09<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0042, Accuracy: 3650/10000 (36.50%)\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.9498 Batch_id=97 Accuracy=48.15: 100%|██████████| 98/98 [00:09<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0039, Accuracy: 4987/10000 (49.87%)\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.9442 Batch_id=97 Accuracy=53.18: 100%|██████████| 98/98 [00:09<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0039, Accuracy: 5100/10000 (51.00%)\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8597 Batch_id=97 Accuracy=55.91: 100%|██████████| 98/98 [00:09<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0038, Accuracy: 5439/10000 (54.39%)\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8142 Batch_id=97 Accuracy=57.86: 100%|██████████| 98/98 [00:09<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0038, Accuracy: 5643/10000 (56.43%)\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.9348 Batch_id=97 Accuracy=59.43: 100%|██████████| 98/98 [00:09<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0038, Accuracy: 5779/10000 (57.79%)\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8277 Batch_id=97 Accuracy=60.44: 100%|██████████| 98/98 [00:09<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0038, Accuracy: 5793/10000 (57.93%)\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8121 Batch_id=97 Accuracy=60.87: 100%|██████████| 98/98 [00:09<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0038, Accuracy: 5743/10000 (57.43%)\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8791 Batch_id=97 Accuracy=61.68: 100%|██████████| 98/98 [00:09<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 6042/10000 (60.42%)\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.7952 Batch_id=97 Accuracy=62.47: 100%|██████████| 98/98 [00:09<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 5952/10000 (59.52%)\n",
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8760 Batch_id=97 Accuracy=63.08: 100%|██████████| 98/98 [00:09<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 6021/10000 (60.21%)\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8124 Batch_id=97 Accuracy=63.04: 100%|██████████| 98/98 [00:10<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 6045/10000 (60.45%)\n",
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8285 Batch_id=97 Accuracy=63.34: 100%|██████████| 98/98 [00:09<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 6065/10000 (60.65%)\n",
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.8716 Batch_id=97 Accuracy=63.82: 100%|██████████| 98/98 [00:09<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 6058/10000 (60.58%)\n",
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.7883 Batch_id=97 Accuracy=64.18: 100%|██████████| 98/98 [00:09<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0037, Accuracy: 5979/10000 (59.79%)\n",
      "\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss=1.7740 Batch_id=31 Accuracy=65.16:  33%|███▎      | 32/98 [00:03<00:07,  8.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     43\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m   train_succeeded, train_processed, train_loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, criterion)\n\u001b[1;32m     45\u001b[0m   train_acc\u001b[39m.\u001b[39mappend(\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m train_succeeded\u001b[39m/\u001b[39mtrain_processed)\n\u001b[1;32m     46\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/era-10/utils.py:40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m train_succeeded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m GetCorrectPredCount(pred, target)\n\u001b[1;32m     41\u001b[0m train_processed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     43\u001b[0m pbar\u001b[39m.\u001b[39mset_description(desc\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain: Loss=\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m0.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Batch_id=\u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m Accuracy=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m*\u001b[39mtrain_succeeded\u001b[39m/\u001b[39mtrain_processed\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/era-10/utils.py:15\u001b[0m, in \u001b[0;36mGetCorrectPredCount\u001b[0;34m(pPrediction, pLabels)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGetCorrectPredCount\u001b[39m(pPrediction, pLabels):\n\u001b[0;32m---> 15\u001b[0m   \u001b[39mreturn\u001b[39;00m pPrediction\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49meq(pLabels)\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 24\n",
    "max_lr_epoch = 5\n",
    "LRMIN = 0.0001\n",
    "LRMAX = 0.01\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LRMIN)  # you can adjust learning rate as needed\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                          max_lr=LRMAX,\n",
    "                                          steps_per_epoch=len(train_loader),\n",
    "                                          epochs=num_epochs,\n",
    "                                          pct_start=max_lr_epoch/num_epochs,\n",
    "                                          div_factor=100,\n",
    "                                          final_div_factor=100,\n",
    "                                          three_phase=False\n",
    "                                          )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # reduction='none' // it can be sum also\n",
    "\n",
    "\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "# lr_finder.range_test(train_loader, end_lr=100, num_iter=100)\n",
    "# lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset() # to reset the model and optimizer to their initial state\n",
    "\n",
    "# losses = np.array(lr_finder.history['loss'])\n",
    "# loss_grad = np.gradient(losses)\n",
    "# lrs = np.array(lr_finder.history['lr'])\n",
    "\n",
    "# # Find loss minimum\n",
    "# min_loss_idx = np.argmin(losses)\n",
    "# min_lr = lrs[min_loss_idx]\n",
    "\n",
    "# # Find steepest slope (most negative gradient)\n",
    "# max_grad_idx = np.argmin(loss_grad)\n",
    "# max_lr = lrs[max_grad_idx]\n",
    "\n",
    "# print(\"Min LR: \", min_lr)\n",
    "# print(\"Max LR: \", max_lr)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  print(f'Epoch {epoch}')\n",
    "  train_succeeded, train_processed, train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "  train_acc.append(100 * train_succeeded/train_processed)\n",
    "  train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "  test_succeeded, test_loss = test(model, device, test_loader, criterion)\n",
    "  test_acc.append(100. * test_succeeded / len(test_loader.dataset))\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "Wu0l7dli4eC9",
    "outputId": "cd1fcdab-0c0f-41dc-d1c9-6b80f9eb7915"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8WZPfXe4iK_"
   },
   "outputs": [],
   "source": [
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "\n",
    "model_children = model.children()\n",
    "for children in model_children:\n",
    "    if type(children) == nn.Sequential:\n",
    "        for child in children:\n",
    "            if type(child) == nn.Conv2d:\n",
    "                counter += 1\n",
    "                model_weights.append(child.weight)\n",
    "                conv_layers.append(child)\n",
    "\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "print(\"conv_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[9]\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.unsqueeze(0)\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "names = []\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(len(outputs))\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "for feature_map in outputs:\n",
    "    feature_map = feature_map.squeeze(0)\n",
    "    gray_scale = torch.sum(feature_map,0)\n",
    "    # gray_scale = feature_map[0]\n",
    "    gray_scale = gray_scale / feature_map.shape[0]\n",
    "    processed.append(gray_scale.data.cpu().numpy())\n",
    "for fm in processed:\n",
    "    print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "for i in range(len(processed)):\n",
    "    a = fig.add_subplot(5, 4, i+1)\n",
    "    imgplot = plt.imshow(processed[i])\n",
    "    a.axis(\"off\")\n",
    "    a.set_title(names[i].split('(')[0], fontsize=10)\n",
    "plt.savefig(str('feature_maps.jpg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first conv layer filters\n",
    "plt.figure(figsize=(5, 4))\n",
    "first_layer_weights = model_weights[0].cpu()\n",
    "for i, filter in enumerate(first_layer_weights):\n",
    "    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
